![MazeGPT](media/logo.svg)

[![linting: pylint](https://img.shields.io/badge/linting-pylint-yellowgreen)](https://github.com/pylint-dev/pylint)

Does some maze generation and stuff. Working on this because I'm bored.
All this thing knows is mazes.

![Transformer](/media/transformer_meme.jpg)

# 🔍 Table of Contents
* 🌎 [Overview](#overview)
* ⏩ [Quickstart](#-quickstart)
* 📗 [Introduction](docs/INTRODUCTION.md#-introduction)
  * ⚠️ [Problem](docs/INTRODUCTION.md#-the-problem)
  * 📐 [Representation](docs/INTRODUCTION.md#-representing-a-maze)
  * 📤 [Tokenizer](docs/INTRODUCTION.md#-tokenizer)
* 🔧 [Getting Started](docs/GETTING_STARTED.md)

## Overview
["Attention Is All You Need"](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) 
was a ground break paper in the world of machine learning in 2017.
The idea of a transformer has dramatically helped reduced the train time while improving the consistency
of attention across long periods of recurrent generation. The company [OpenAI](https://openai.com/) has two models ChatGPT and DALL·E both implementing transformers to achieve 
incredible results. 

The objective of this research project was to implement a transformer model for generating mazes. 
While there are numerous existing maze algorithms that perform well, they tend to produce recurrent patterns despite 
being seeded randomly. The goal is to achieve mazes that are more random and chaotic in nature and mimic human behavior.



## 🔧 Quickstart
TODO

## Authors
- Noah Hein ([@noah-hein](https://github.com/noah-hein))



